import 'dotenv/config';
import { OpenAI } from 'openai';
import axios from 'axios';
import colors from 'colors';
import { exec } from 'child_process';

const client = new OpenAI();

async function getWeatherDetailsByCity(cityname = '') {
    const url = `https://wttr.in/${cityname.toLowerCase()}?format=%C+%t`;
    try {
        const { data } = await axios.get(url, { responseType: 'text' });
        return `The current weather in ${cityname} is ${data}`;
    } catch (err) {
        return `Error: ${err.message}`
    }
}

// getWeatherDetailsByCity('Delhi').then(console.log).catch(console.error);


async function gitGithubInfoByUsername(username = '') {
    const url = `https://api.github.com/users/${username}`;
    try {
        const { data } = await axios.get(url);
        return JSON.stringify(data);
    } catch (err) {
        return `Error: ${err.message}`
    }
}

async function executeCommand(command = '') {
    return new Promise((resolve, reject) => {
        exec(command, (error, data) => {
            if (error) {
                reject(`Error: ${error.message}`);
            } else {
                if (data && data.trim().length > 0) {
                    resolve(`Command executed successfully: ${data.trim()}`);
                } else {
                    resolve('Command executed successfully (no output)');
                }
            }
        });
    });
}

const TOOL_MAP = {
    getWeatherDetailsByCity: getWeatherDetailsByCity,
    gitGithubInfoByUsername: gitGithubInfoByUsername,
    executeCommand: executeCommand
}

async function main() {
    // These API calls are stateless and can be made in parallel.
    // The client will handle rate limits and retries automatically.

    // Chain-of-thought
    // The model is guided to think through a problem step-by-step before arriving at a final answer.
    // This approach helps in breaking down complex problems and ensures a more thorough reasoning process.
    // LLM as a judge techinique is used to verify the thoughts of the primary LLM.
    // Here, Gemini is used as a secondary model to validate the reasoning steps of the primary model (OpenAI).
    // This helps in ensuring the accuracy and reliability of the responses generated by the primary model.
    const SYSTEM_PROMPT = `
        You are an AI assistant who works on START, THINK and OUTPUT format. For a given user query, you first START by acknowledging the query, then THINK by reasoning step-by-step about the best way to address the query, and finally OUTPUT the final answer. You should always keep thinking and thinking before giving the actual output.
        Also, before outputting the final answer to the user, you must check once if everything is correct.
        You also have list of available tools that you can call based on user query.
        For every tool call that you make, wait for the OBSERVATION from the tool which is the response from the tool that you called.

        Available Tools:
        1. getWeatherDetailsByCity(cityname: string): This tool provides the current weather information for a specified city. You can use this tool when the user asks about the weather in a particular location.
        2. gitGithubInfoByUsername(username: string): This tool fetches detailed information about a GitHub user based on their username. You can use this tool when the user requests information about a specific GitHub profile.
        3. executeCommand(command: string): This tool executes a given shell command on the server and returns the output. You can use this tool when the user wants to run a specific command and see its result.

        Rules:
        - Strictly follow the START, THINK OBSERVE and OUTPUT format.
        - Strictly follow the output JSON format.
        - Always return a valid JSON. Never break the JSON format.
        - Always perform only one step at a time and wait for other step.
        - Always make sure to do multiple steps of thinking before giving out output.
        - For every tool call always wait for the OBSERVE which contains the output from tool that you called. 

        Output JSON format:
        {"step": "START/THINK//OBSERVE/TOOL/OUTPUT", "content": "string", "tool_name": "string", "input": "string"}

        Example:
        User: Can you tell me the current weather in Delhi?
        Assistant: {"step": "START", "content": "Sure! Let's start by understanding the problem."}
        Assistant: {"step": "THINK", "content": "The user wants to know the current weather in Delhi."}
        Assistant: {"step": "THINK", "content": "Let me check if I have a tool that can provide weather information."}
        Assistant: {"step": "THINK", "content": "I can use the getWeatherDetailsByCity tool to get the weather information for Delhi."}
        Assistant: {"step": "TOOL", "input": "Delhi", "tool_name": "getWeatherDetailsByCity"}
        Developer: {"step": "OBSERVE", "content": "The current weather in Delhi is Partly cloudy +30°C"}
        Assistant: {"step": "THINK", "content": "I have received the weather information for Delhi."}
        Assistant: {"step": "OUTPUT", "content": "The current weather in Delhi is Partly cloudy +30°C."}
    `;
    const messages = [
        {
            role: 'system',
            content: SYSTEM_PROMPT,
        },
        {
            role: 'user',
            // content: 'What is the weather of Delhi, Mumbai, Kolkata and Chennai? Also tell me the average weather of all the cities in India.',
            // content: 'Can you provide detailed information about the GitHub user "shashi162003"?'
            content: 'Hey, create a folder todo_app and create a simple todo app using html, css and javascript.'
        }
    ];

    while (true) {
        const response = await client.chat.completions.create({
            model: 'gpt-4.1-mini',
            messages: messages,
        });
        const rawContent = response.choices[0].message.content;
        const parsedContent = JSON.parse(rawContent);

        if (parsedContent.step === 'START') {
            console.log(`Beginning: ${parsedContent.content}`);
            messages.push({ role: 'assistant', content: rawContent });
            continue;
        }
        else if (parsedContent.step === 'THINK') {
            console.log(`Thinking: ${parsedContent.content}`);
            messages.push({ role: 'assistant', content: rawContent });
            continue;
        }
        else if (parsedContent.step === 'TOOL') {
            const toolToCall = TOOL_MAP[parsedContent.tool_name];
            if (toolToCall) {
                const observation = await toolToCall(parsedContent.input);
                const observationMessage = {
                    role: 'developer',
                    content: JSON.stringify({
                        step: 'OBSERVE',
                        content: `The current weather in ${parsedContent.input} is ${observation}`
                    }),
                };
                messages.push(observationMessage);
                console.log(`Tool: ${parsedContent.tool_name} called with input: ${parsedContent.input}. Observation: ${observation}`.blue);
                continue;
            }
            else {
                console.log(`Error: Tool ${parsedContent.tool_name} not found.`);
                messages.push({
                    role: 'developer',
                    content: `Error: Tool ${parsedContent.tool_name} not found.`
                });
                continue;
            }
        }
        else if (parsedContent.step === 'OUTPUT') {
            console.log(`Output: ${parsedContent.content}`);
            messages.push({ role: 'assistant', content: rawContent });
            break;
        }
    }
}

main();